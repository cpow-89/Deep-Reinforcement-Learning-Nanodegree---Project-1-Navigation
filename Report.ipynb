{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Deep Q-Learning for Multilayer Perceptron for solving Deep Reinforcement Learning Nanodegree - Project 1: Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Reinforcement Learning Framework:\n",
    "- reinforcement learning (RL) framework is characterized by an agent learning to interact with its environment\n",
    "- at each time step, the agent receives the environment's state \n",
    "- the agent must choose an appropriate action in response\n",
    "- at the next time step, the agent receives a reward and a new state\n",
    "- the agent has the goal to maximize the expected cumulative reward\n",
    "\n",
    "\n",
    "### Policy:\n",
    "- we can distinguish between deterministic and stochastic policy\n",
    "##### Deterministic policy:\n",
    "- a mapping from state to action\n",
    "    - $\\pi : S \\to A$\n",
    "    - for each state $s \\in S$ it yields the action $a \\in A$ that the agent will choose while in state $s$\n",
    "####  Stochastic policy:\n",
    "- a mapping from state to action probability \n",
    "    - $\\pi : S \\times A \\to [0,1]$\n",
    "    - for each state $s \\in S$ it and action $a \\in A$ it yields the probability $\\pi(a|s)$ that the agent chooses action $a$ while in state $s$\n",
    "\n",
    "### State-Value Functions:\n",
    "\n",
    "- state-value function for a policy $\\pi$ is denoted $v_\\pi$\n",
    "- $v_\\pi = \\mathbb{E_\\pi}[G_t | S_t = s]$\n",
    "    - for each state $s \\in S$, it yields the expected return if the agent starts in state $s$ and then uses the policy $\\pi$ to choose its actions $a$ for all remaining time steps\n",
    "    - $\\mathbb{E_\\pi}$ is the expected value of a random variable, given that the agent follows policy $\\pi$\n",
    "    \n",
    "### Action-Value Functions:\n",
    "\n",
    "- action-value function for a policy  $\\pi$ is denoted $q_\\pi$\n",
    "- $q_\\pi(s, a) = \\mathbb{E_\\pi}[G_t | S_t = s, A_t = a]$\n",
    "    - for each state $s \\in S$ and action $a \\in A$ tuple, it yields the expected return if the agent starts in state $s$, takes action $a$, and then follows the policy for all remaining time steps\n",
    "    - $\\mathbb{E_\\pi}$ is the expected value of a random variable, given that the agent follows policy $\\pi$\n",
    "    \n",
    "### Temporal Difference:\n",
    "- $TD_t(a_t, s_t) = r_t + \\gamma max_a(Q(a, s_{t+1})) - Q(a, s_t)$ <p>\n",
    "$r_t$: reward $r_t$ obtained by playing the action $a_t$ in the state $s_t$<br>\n",
    "$\\gamma$: discount factor(hyperparameter)<br>\n",
    "$max_a(Q(a, s_{t+1}))$: Q-value of the best action played in the state $s_{t+1}$<br>\n",
    "$Q(a, s_t)$: Q-value of the action $a_t$ played in the state $s_t$\n",
    "    \n",
    "- use the temporal difference $TD_t(a_t, s_t)$ to update $Q(a, s_t)$ \n",
    "    - $Q(a, s_t) = Q(a, s_t) + \\alpha * TD_t(a_t, s_t) $ \n",
    "    - the action value function is updated after every time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Generalization-of-Q-learning-with-neural-network.png \"Q_Learning_Network\")\n",
    "[Image Source](https://www.researchgate.net/profile/Jason_Gu2/publication/251898588/figure/fig1/AS:358420172558336@1462465436051/Generalization-of-Q-learning-with-neural-network.png)\n",
    "\n",
    "- in deep Q-learning, we use an artificial neural network as q value function\n",
    "    - the input to the network is the environment states\n",
    "    - the output of the network is a q-value for all possible actions\n",
    "    - we can now select the action based on this q-values\n",
    "        - for example by using an $\\epsilon$-greedy policy\n",
    "    - the network is updated by using the temporal difference method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implemented Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Q-Targets:\n",
    "- in vanilla Q-Learning, we are updating a guess for the q values with a guess\n",
    "- this can potentially lead to harmful correlations\n",
    "- we can break these correlations by using fixed Q-Targets meaning we are using two separated networks\n",
    "    - new updaterule: $\\Delta w = \\alpha * (r_t + \\gamma max_a(Q(a, s_{t+1}, w^-)) - Q(a, s_t, w)) \\nabla_w Q(a, s_t, w)$\n",
    "    - $w^-$ are the weights of a separate target network that are not changed during the learning step\n",
    "        - target Q-Network weights are updated less often(or more slowly) than the Q-Network\n",
    "        \n",
    "### Experience Replay:\n",
    "- deals with the problem that the sequence of experience tuples can be highly correlated\n",
    "- by learning from this experience tuples in sequential order, we run into the risk of getting swayed by the effects of this correlation\n",
    "- we can handle this problem by saving the last $m$ transitions\n",
    "    - $m$ is a large number\n",
    "    - a transition (or experience) is often defined as a tuple in the form of (state, action, reward, next_state, done)\n",
    "- the pack of the last transition is called experience replay(or replay buffer)\n",
    "- from this replay buffer, we take random batches of transitions to make updates to the network\n",
    "    - this randomness breaks the correlation\n",
    "    - this also allows us to learn more from individual tuples multiple times\n",
    "\n",
    "### Reward Clipping:\n",
    "- deals with the problem that most task has a different reward(score) scale\n",
    "- this difference makes training unstable\n",
    "- to avoid these problems, we can clip all positive rewards to a range between -1 and 1\n",
    "- this technique also reduces the impact of extreme observations\n",
    "- whether or not reward clipping is helpful depends on the given task\n",
    "\n",
    "### Gradient Clipping:\n",
    "- deals with the problem of exploding gradients\n",
    "    - gradient gets exponentially large from being multiplied by numbers larger than 1\n",
    "- gradient clipping puts the gradients in a range between a min and max value to avoid this problem\n",
    "\n",
    "### Double Deep Q-Learning:\n",
    "- addresses the problem of overestimation of the Q-Values\n",
    "- the Q-Values depend a lot on what actions have been already tried and what states have been explored\n",
    "- that means that choosing the right max value can be error-prone especially in the early stages\n",
    "    - that is because the Q-values are still evolving and we might need more information to figure out the best action\n",
    "    - we are choosing the max value among a set of noisy numbers which leads to the overestimation\n",
    "- to solve this problem, we select the best action using one set of parameters $w$ but evaluate it using a different set of parameters $w^-$\n",
    "    - we can use our already introduced target Q-Network for this\n",
    "- that means we have two separate function approximators that must agree on the best action\n",
    "    - if $w$ picks an action that is not the best according to $w^-$ then the estimated Q-Value is not that high\n",
    "    \n",
    "\n",
    "### Dueling Networks:\n",
    "\n",
    "![alt text](images/dueling_dqn.png \"Dueling_Q_Learning_Network\")\n",
    "[Image Source](https://coach.nervanasys.com/algorithms/value_optimization/dueling_dqn/index.html)\n",
    "\n",
    "- the core idea of dueling networks is to use a specialized Dueling Q Head to separate $Q$ to an $A$ (advantage) stream and a $V$ (state value) stream\n",
    "    - the streams may share some layers at the beginning then branch off with their own fully connected layers\n",
    "    - this allows the network to differentiate actions from one another better\n",
    "- the reason for this is that the value of most states do not vary a lot across actions\n",
    "    - we can start learning the state-value even if only a single action has been taken at this state\n",
    "- this leads to faster learning, but we still need to capture the difference that actions make in each state\n",
    "    - the advantage stream realizes this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Learning algorithm:\n",
    "\n",
    "Step 1: Initialization:\n",
    "- initialize environment\n",
    "- initialize agent\n",
    "    - the agent gets a config file\n",
    "    - the agent sets up the q-network(architecture described in 1.2 Network Architecture)\n",
    "    - the agent sets up the q_target-network(same architecture as the q-network)\n",
    "    - the agent sets up the optimizer(RMSprop)\n",
    "        - the learning rate is specified in the config file -> [\"agent\"][\"learning_rate\"]\n",
    "        - Note: I choose RMSprop over Adam because Adam started to oscillate close to the end of the training session\n",
    "    - the agent sets up the criterion(Mean square error loss)\n",
    "    - the agent initializes the experience replay buffer\n",
    "        - the buffer size is specified in the config file -> [\"agent\"][\"gamma\"]\n",
    "\n",
    "Step 2: Training:\n",
    "- if training is set to true the agent will go through a learning session\n",
    "\t- the is training flag is specified in the config file -> config[\"train\"][\"run_training\"] \n",
    "\n",
    "Training Session:\n",
    "- run n numbers of training epochs\n",
    "    - one epoch is equal to one episode in the environment(the banana project is an episodic task)\n",
    "    - an epoch is closed if the environment send the \"done\" flag\n",
    "- for each episode:\n",
    "    - reset the environment and get the initial state\n",
    "    - while environment does not set the done flag to true -> run learning steps\n",
    "        - let the agent choose an action based on the given state\n",
    "            - the agent calculates all q-values for the given state using its q-network\n",
    "            - based on an \"epsilon-greedy action selection\" the agent chooses an action\n",
    "                - we calculate a random number between 0 and 1\n",
    "                - if the random number is higher then epsilon, then we choose the action with the highest q-value given by the q-network(exploitation) \n",
    "                - else we pick a random action(exploration)\n",
    "        - take that action in the environment\n",
    "        - get the next_state, reward and done flag as the return from the environment\n",
    "        - save a tuple of state, action, reward, next_state, done to the agents experience buffer\n",
    "        - run one learn step\n",
    "            - check if the learning criteria are fulfilled\n",
    "                - the buffer includes the minimal amount of memory(at least as much as our batch size)\n",
    "                - the current step must be a learning step(we only learn every x steps\n",
    "                    - this leads to more stable learning \n",
    "                    - step size is specified in the config file -> [\"agent\"][\"update_rate\"] \n",
    "                - if learning criteria are not fulfilled, we ignore the current learning step\n",
    "                - if learning criteria are fulfilled, we run the learning step\n",
    "            - sample one batch of experiences from the experience buffer\n",
    "                - batch size is specified in the config file -> [\"train\"][\"batch_size\"]\n",
    "                - the experience buffer returns a random batch of tuples in the form of states, actions, rewards, next_states, dones\n",
    "            - calculate the loss for the current experience batch\n",
    "                - use the q-network to calculate the q-values for all states, action pairs in the experience tuple\n",
    "                    - the result is called q_eval in the code\n",
    "                - use the q-network to predict the action with the highest q-value for all next_states in the experience tuple\n",
    "                    - the result is called q_argmax in the code\n",
    "                - use the q-target-network to calculate the q-values for all next_states, q_argmax pairs\n",
    "                    - the result is called q_next in the code\n",
    "                - use the q-target-network output to calculate the q_target values\n",
    "                    - if done is false -> q_target = reward + gamma * q_next \n",
    "                        - gamma is specified in the config file -> [\"agent\"][\"gamma\"] \n",
    "                    - if done is true -> q_target = reward\n",
    "                - calculate the loss of the q-network using the agent's criterion function\n",
    "                    - loss = criterion(q_eval, q_target)\n",
    "             - update the weights of the q-network based on the calculated loss using the agent's optimizer\n",
    "             - update the weights of the q-target network using a soft update\n",
    "                 - make one small step towards the weight values of the q-network weights\n",
    "                 - the step size is based on tau \n",
    "                     - tau is specified in the config file -> [\"agent\"][\"tau\"]\n",
    "                 - this leads to more stable learning \n",
    "        - set state to next_state\n",
    "    - update epsilon value\n",
    "        - decrease epsilon using the epsilon decay factor until it is equal to epsilon low\n",
    "\t- epsilon decay und epsilon low are specified in the config file -> config[\"train\"][\"epsilon_decay\"], config[\"train\"][\"epsilon_low\"]\n",
    "        - this leads to more exploration at the beginning of the training session and more exploitation towards the end\n",
    "\n",
    "##### 4.2 Network Architecture\n",
    "- a dueling Networks architecture is used\n",
    "\n",
    "![alt text](images/network.png \"Network Architecture\")\n",
    "- both q-network and q-target-network are using this architecture\n",
    "\n",
    "##### 4.3 Hyperparameters and configurations:\n",
    "\n",
    "[\"general\"][\"state_size\"] = 37<br>\n",
    "- state size is the size of the state vector provided by the environment at each timestep<br>\n",
    "- state size also determines the number of input nodes for the q-network and q-target-network<br>\n",
    "\n",
    "[\"general\"][\"action_size\"] = 4<br>\n",
    "- action size is the number of actions the agent can choose to interact with the environment<br>\n",
    "- action size also determines the number of output nodes for the q-networkand q-target-network<br>\n",
    "\n",
    "[\"general\"][\"average_score_for_solving\"] : 15.0<br>\n",
    "- average_score_for_solving is the average reward (over 100 episodes) needed to solve the environment<br>\n",
    "    - Note: the environment is solved with average reward over +13 (over 100 episodes), but I set it to +15 cause my agent shows a good learning curve during testing<br>\n",
    "    \n",
    "[\"train\"][\"nb_episodes\"] = 1800<br>\n",
    "- max number of episodes during training session<br>\n",
    "    - the environment should be solved under 1800 episodes<br>\n",
    "    \n",
    "[\"train\"][\"batch_size\"] = 256<br>\n",
    "- number of states, actions, rewards, next_states, dones tuples sampled from the experience buffer during training<br>\n",
    "\n",
    "[\"train\"][\"epsilon_high\"] = 1.0<br>\n",
    "- initial epsilon value<br>\n",
    "    - 1.0 means the agent choose first actions randomly(100% exploration)<br>\n",
    "    \n",
    "[\"train\"][\"epsilon_low\"] = 0.01<br>\n",
    "- the lowest value epsilon can become<br>\n",
    "    - 0.01 means we want the agent to exploit his knowledge most of the time, but we also have a minimal change of exploration during later episodes<br>\n",
    "    \n",
    "[\"train\"][\"epsilon_decay\"] = 0.995<br>\n",
    "- the rate in which epsilon decrease from epsilon_high to epsilon_low at each episode<br>\n",
    "    - we want this value to be close to 1 to get a slow but constant decrease<br>\n",
    "    \n",
    "[\"agent\"][\"learning_rate\"] = 0.001<br>\n",
    "- the rate in which the agent learns(how big are the weight update steps)<br>\n",
    "    - huge values lead to fast learning but will probably overshoot the optimum<br>\n",
    "    - small values might lead to very slow learning <br>\n",
    "    - I choose 0.001 cause higher values led to oscillation of the received reward<br>\n",
    "    \n",
    "[\"agent\"][\"gamma\"] = 0.99<br>\n",
    "- is a decay factor for future rewards meaning received rewards currently should have more value than uncertain future rewards<br>\n",
    "- this value should be close to 1 cause we only took one step into the future into acount<br>\n",
    "\n",
    "[\"agent\"][\"tau\"] = 0.001<br>\n",
    "- this value determines the step size of the soft q-network to q-target network weight update<br>\n",
    "- the value should be close to one to get a more stable learning process<br>\n",
    "\n",
    "[\"agent\"][\"update_rate\"] = 4<br>\n",
    "- the frequenz in wich a learning step is done<br>\n",
    "- a value of 4 means we run one update step every 4 episodes<br>\n",
    "\n",
    "[\"buffer\"][\"size\"] = 100000<br>\n",
    "- the number of experience tuples we can save to our experience replay buffer<br>\n",
    "- this value should be high to save as much experience as possible<br>\n",
    "\n",
    "[\"model\"][\"fc1_nodes\"] = 256<br>\n",
    "- the number of nodes for the first fully connected layer<br>\n",
    "- 256 is choosen cause it led to the fastest learning curve while still obtained a smooth learning curve<br>\n",
    "\n",
    "[\"model\"][\"fc2_adv\"] = 256<br>\n",
    "- the number of nodes for the advantage stream of the dueling Network<br>\n",
    "- 256 is choosen cause it led to the fastest learning curve while still obtained a smooth learning curve<br>\n",
    "\n",
    "[\"model\"][\"fc2_val\"] = 256<br>\n",
    "- the number of nodes for the state value stream of the Dueling Network<br>\n",
    "- 256 is chosen cause it led to the fastest learning curve while still obtained a smooth learning curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Project Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import helper\n",
    "import sessions\n",
    "from dqn_agent import Agent\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.94\n",
      "Episode 200\tAverage Score: 1.83\n",
      "Episode 300\tAverage Score: 3.87\n",
      "Episode 400\tAverage Score: 5.82\n",
      "Episode 500\tAverage Score: 8.14\n",
      "Episode 600\tAverage Score: 9.22\n",
      "Episode 700\tAverage Score: 9.973\n",
      "Episode 800\tAverage Score: 11.02\n",
      "Episode 900\tAverage Score: 11.72\n",
      "Episode 1000\tAverage Score: 12.61\n",
      "Episode 1100\tAverage Score: 13.27\n",
      "Episode 1200\tAverage Score: 13.32\n",
      "Episode 1300\tAverage Score: 14.10\n",
      "Episode 1400\tAverage Score: 14.05\n",
      "Episode 1500\tAverage Score: 14.43\n",
      "Episode 1540\tAverage Score: 15.06\n",
      "Environment solved in 1440 episodes!\tAverage Score: 15.06\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(os.path.join(\".\", \"configs\", \"Extended_Dqn_Banana_config.json\"), \"r\") as read_file:\n",
    "    config = json.load(read_file)\n",
    "\n",
    "env = UnityEnvironment(file_name=os.path.join(*config[\"general\"][\"env_path\"]))\n",
    "agent = Agent(config=config)\n",
    "\n",
    "if config[\"train\"][\"run_training\"]:\n",
    "    scores = sessions.train(agent, env, config)\n",
    "    helper.plot_scores(scores)\n",
    "    agent.save()\n",
    "else:\n",
    "    agent.load()\n",
    "    sessions.test(agent, env)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Ideas for Future Work\n",
    "\n",
    "##### 1. Training Time: \n",
    "- add Prioritized Experience Replay to minimize the total training time\n",
    "\n",
    "##### 2. Stability:\n",
    "- the agent sometimes starts oscillating between two actions\n",
    "    - this problem might be solvable by introducing some noise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
